# Awesome-Generation4WorldModel-papers

![Visitors](https://api.infinitescript.com/badgen/count?name=visitworld123/Awesome-Generation4WorldModel-papers&ltext=Visitors)

Collect awesome papers about establishing a world model with the help of generative abilities, like text, image, and video.

Feel free to request PR, issue, or e-mail me if you want to add more missing papers!üòäüòäüòä

If this repo is useful for your research, please consider star‚≠ê‚≠ê or share with others~~üòÉüòÉüòÉ

Still collecting~




## Survey/Benchmark
|  	| paper 	| Affiliation|Other useful link 	|
|---	|---	|---	|---|
|7 Mar 2023|Foundation models for decision making: Problems, methods, and opportunities|Google Research, UCB, MIT, University of Alberta|[paper](https://arxiv.org/pdf/2303.04129)
|  27 Feb 2024	| Video as the New Language for Real-World Decision Making 	| DeepMind, MIT, UCB 	|[paper](https://arxiv.org/pdf/2402.17139)
| 6 May 2024 	| Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond  	| GigaAI, CAS <br> NUS, Shanghai AI Lab|[paper](https://arxiv.org/pdf/2405.03520) <br>[GitHub](https://github.com/GigaAI-research/General-World-Models-Survey)	|
|15 Nov 2023| Imagine the Unseen World: A Benchmark for Systematic Generalization in Visual World Models|KAIST, Rutgers University, EPFL, DeepMind|[paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/58af908d6293810f1a29e69bf723dc48-Paper-Datasets_and_Benchmarks.pdf)<br>[Project page](https://systematic-visual-imagination.github.io/)<br>[GitHub](https://github.com/systematic-visual-imagination/svib)
|7 Jun 2024|Towards Generalist Robot Learning from Internet Video: A Survey|UCL, Weco AI, MIT|[paper](https://arxiv.org/pdf/2404.19664)|


## Video Generation for World Model (universal planner)
|  	| paper 	| Affiliation|Other useful link 	|
|---	|---	|---	|---|
|12 Oct 2023|Learning to Act from Actionless Videos through Dense Correspondences|National Taiwan University, MIT|[paper](https://arxiv.org/pdf/2310.08576)<br>[Project page](https://flow-diffusion.github.io/)<br>[GitHub](https://github.com/flow-diffusion/AVDC)|
|16 Oct 2023|Video Language Planning|DeepMind, MIT, UCB|[paper](https://arxiv.org/pdf/2310.10625)<br> [Project page](https://video-language-planning.github.io/)<br>[GitHub](https://github.com/video-language-planning/vlp_code)
|27 Oct 2023|Pre-training Contextualized World Models with In-the-wild Videos for Reinforcement Learning|THU|[paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/7ce1cbededb4b0d6202847ac1b484ee8-Paper-Conference.pdf)<br>[GitHub](https://github.com/thuml/ContextWM)
|20 Nov 2023|Learning Universal Policies via Text-Guided Video Generation|MIT, DeepMind, UCB|[paper](https://proceedings.neurips.cc/paper_files/paper/2023/file/1d5b9233ad716a43be5c0d3023cb82d0-Paper-Conference.pdf) <br> [Project page](https://universal-policy.github.io/) <br> [GitHub](https://github.com/flow-diffusion/AVDC)|
| 27 Nov 2023	| Drivedreamer: Towards real-world-driven world models for autonomous driving	| GigaAI, THU|[paper](https://arxiv.org/pdf/2309.09777) <br> [Project page](https://drivedreamer.github.io/)	|
|13 Jan 2024|Learning Interactive Real-World Simulators|UCB, MIT, DeepMind, University of Alberta|[paper](https://arxiv.org/pdf/2310.06114)<br> [Project page](https://universal-simulator.github.io/unisim/)
|18 Jan 2024|WorldDreamer: Towards General World Models for Video Generation via Predicting Masked Tokens|GigaAI, THU|[paper](https://arxiv.org/pdf/2401.09985) <br> [Project page](https://world-dreamer.github.io/)
|16 Feb 2024|Using Left and Right Brains Together: Towards Vision and Language Planning|SUSTC, MSRA, HKUST, XJTU, CityU|[paper](https://arxiv.org/pdf/2402.10534)|
|5 Mar 2024|Why Not Use Your Textbook Knowledge-Enhanced Procedure Planning of Instructional Videos|MBZUAI, NEC, University of Auckland, Weizmann Institute of Science| [paper](https://arxiv.org/pdf/2403.02782)<br>[GitHub](https://github.com/Ravindu-Yasas-Nagasinghe/KEPP)|
|1 Apr 2024|Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion|Waabi, University of Toronto|[paper](https://arxiv.org/abs/2311.01017)<br>[Project page](https://waabi.ai/copilot-4d/)
|  11 Apr 2024	| Drivedreamer-2: Llm-enhanced world models for diverse driving video generation 	| GigaAI, CASIA	|[paper](https://arxiv.org/pdf/2403.06845) <br> [Project page](https://drivedreamer2.github.io/) 	|
|18 Apr 2024|RoboDreamer: Learning Compositional World Models for Robot Imagination|HKUST, MIT, UCB|[paper](https://arxiv.org/pdf/2404.12377)<br> [Project page](https://robovideo.github.io/) <br>[GitHub](https://github.com/rainbow979/robodreamer)
|May 23 2024|Pandora: Towards General World Model with Natural Language Actions and Video States|Maitrix, UCSD, MBZUAI|[paper](https://world-model.maitrix.org/assets/pandora.pdf)<br>[Project page](https://world-model.maitrix.org/)<br>[GitHub](https://github.com/maitrix-org/Pandora?tab=readme-ov-file)
| 2 Jun 2024|iVideoGPT: Interactive VideoGPTs are Scalable World Models|THU, Huawei, Tianjin|[paper](https://arxiv.org/pdf/2405.15223)<br> [Project page](https://thuml.github.io/iVideoGPT/)<br>[GitHub](https://github.com/thuml/iVideoGPT)
